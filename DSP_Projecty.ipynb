{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSP_Projecty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/jeh+xcJ94z5NlkGWxoTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeaganChasenski/Hand-Written-Digit-Prediction-/blob/main/DSP_Projecty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61Z4p1Va5Uwx"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time \n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Hyper Parameters\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9 \n",
        "epochs = 30"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjG1loI2c2u0"
      },
      "source": [
        "def load_data():\n",
        "    print(\"loading data...\")\n",
        "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,)),])\n",
        "\n",
        "    trainset = datasets.MNIST(r'..\\input\\MNIST', download=True, train=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "    testset = datasets.MNIST(r'..\\input\\MNIST', download=True, train=False, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "    # creating a iterator\n",
        "    dataiter = iter(trainloader) \n",
        "    # creating images for image and lables for image number (0 to 9) \n",
        "    images, labels = dataiter.next() \n",
        "\n",
        "    # Return images and labels created from the data iterator\n",
        "    return images, labels, trainloader, testloader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h55GPLPhZQg0"
      },
      "source": [
        "def build_Model():\n",
        "\n",
        "    print(\"building model... \\n\")\n",
        "    # Firstly we know that the first input layer with need 784 neurons ( 28 pixels X 28 pixels)\n",
        "    # nn.Sequetial from PyTorch allows a tensor to passed sequentially through operations\n",
        "\n",
        "    model=nn.Sequential(nn.Linear(784,256), # 1st layer: 784 input 128 output\n",
        "            nn.ReLU(),          # ReLu activation for 1st layer\n",
        "            nn.Linear(256,128),  # 2nd Layer: 128 Input and 64 output\n",
        "            nn.ReLU(),          # ReLu activation for 2nd layer\n",
        "            nn.Linear(128,10),   # 3rd Layer: 64 Input and 10 outout for (0-9)\n",
        "            nn.LogSoftmax(dim=1) # log softmax for the probablities of the output layer (3rd)\n",
        "            ) \n",
        "\n",
        "    # Display model used\n",
        "    print(\"Model built: \")\n",
        "    print(\"\\t 1) 1st layer: Linear (in_ features = 784, out_features = 128)\")\n",
        "    print(\"\\t \\t Activation function for 1st layer = ReLU()\")\n",
        "    print(\"\\t 2) 2nd layer: Linear (in_ features = 128, out_features = 64)\")\n",
        "    print(\"\\t \\t Activation function for 2nd layer = ReLU()\")\n",
        "    print(\"\\t 3) 3rd layer: Linear (in_ features = 64, out_features = 10)\")\n",
        "    print(\"\\t \\t Activation function for 3rd layer = LogSoftmax() \\n\")\n",
        "\n",
        "    # Return model\n",
        "    return model\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46RywARZrN2"
      },
      "source": [
        "def loss_function(images, labels, train_loader, model):\n",
        "    print(\"defining loss function...\")\n",
        "\n",
        "    # Define cross Entropy Loss\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    # Get next image and label from the traini_loader using the iterator \n",
        "    images, labels = next(iter(train_loader))\n",
        "    # Flatten to shape we can perform calculations on\n",
        "    images = images.view(images.shape[0], -1)\n",
        "\n",
        "    #log probabilities\n",
        "    logps = model(images) \n",
        "    #calculate the loss\n",
        "    loss = loss_function(logps, labels) \n",
        "    \n",
        "    # Return loss calculation and the cross entropy function\n",
        "    return loss, loss_function\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dAtgBr0afwP"
      },
      "source": [
        "def grad_weights(loss, model):\n",
        "\n",
        "    # to calculate gradients of parameters \n",
        "    # Use backward pass from PyTorch\n",
        "    loss.backward()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etli0Ci2as4O"
      },
      "source": [
        "def train(model, train_loader, loss_function):\n",
        "    print(\"Training neural network...\")\n",
        "\n",
        "    # Optimiser using stochastic gradient descent\n",
        "    optimiser = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "    # Load images and labels through iterator\n",
        "    images, labels = next(iter(train_loader))\n",
        "    images.resize_(64, 784)\n",
        "\n",
        "    # Clear gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(images)\n",
        "    loss = loss_function(output, labels)\n",
        "\n",
        "    # backward pass to update weights\n",
        "    loss.backward()\n",
        "\n",
        "    # Get start time\n",
        "    time0 = time()\n",
        " \n",
        "\n",
        "    # Loop for each training epoch\n",
        "    for e in range(epochs):\n",
        "        loss_total = 0\n",
        "\n",
        "        # Loop for each image and assoicated label in the train_loader\n",
        "        for images, labels in train_loader:\n",
        "            # Flatenning MNIST images with size [64,784]\n",
        "            images = images.view(images.shape[0], -1) \n",
        "        \n",
        "            # Set gradient = 0 for each epoch\n",
        "            optimiser.zero_grad()\n",
        "            \n",
        "            # Model for each image\n",
        "            output = model(images)\n",
        "            \n",
        "            # calculate loss\n",
        "            loss = loss_function(output, labels)\n",
        "            \n",
        "            # Learn from backpropagating\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimizes weights \n",
        "            optimiser.step()\n",
        "            \n",
        "            # calculate the running loss total\n",
        "            loss_total += loss.item()\n",
        "\n",
        "        # Display for each epoch the running loss\n",
        "        print(\"Epoch {} -> Training loss = {}\".format(e, (loss_total/len(train_loader))))\n",
        "\n",
        "    # Display total runnning time of training.\n",
        "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuRQBzCKa6V9"
      },
      "source": [
        "def validate(test_loader, model):\n",
        "    print(\"\")\n",
        "    print(\"Validating model...\")\n",
        "\n",
        "    # Initiates counters\n",
        "    correct_counter = 0\n",
        "    total_counter = 0\n",
        "\n",
        "    # for each image bacth in the testing_loader\n",
        "    for images,labels in test_loader:\n",
        "        # Loop through the image batch\n",
        "        for i in range(len(labels)):\n",
        "            # load the image at i counter\n",
        "            img = images[i].view(1, 784)\n",
        "\n",
        "            # Calculate the log probability using no gradient \n",
        "            # Speed up the process using no gradient\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Call image and pass in image\n",
        "                logps = model(img)\n",
        "\n",
        "            # Convert to natural number by taking exponent of log\n",
        "            ps = torch.exp(logps)\n",
        "            # Creates an array of size 10, for probability of each digit\n",
        "            probab = list(ps.numpy()[0])\n",
        "\n",
        "            # Get the max, which will be the classification\n",
        "            pred_label = probab.index(max(probab))\n",
        "            # Get ground truth for current image\n",
        "            true_label = labels.numpy()[i]\n",
        "\n",
        "            # Compare ground truth and prediction\n",
        "            if(true_label == pred_label):\n",
        "                # If = then prediciton was correct\n",
        "                # Add 1 to counter\n",
        "                correct_counter += 1\n",
        "            # Increase total counter\n",
        "            total_counter += 1\n",
        "\n",
        "    # Display the accuracy of as correct_counter / totoal_counter\n",
        "    print(\"Number Of Images Tested =\", total_counter)\n",
        "    print(\"Model Accuracy =\", (correct_counter/total_counter))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP3_Pe--bC6C"
      },
      "source": [
        "def classify_image(path, model, ):\n",
        "\n",
        "    # Open Image using PIL Image\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # Create a transform for the image\n",
        "    # Convert it to a tensor\n",
        "    t = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Grayscale(num_output_channels=1)\n",
        "    ])\n",
        "     \n",
        "    # Transform the image\n",
        "    img_tr = t(img)\n",
        "    # Flatten to 1D by 784 pixels\n",
        "    img_tr_new = img_tr.view(1, 784)\n",
        "    \n",
        "    plt.imshow(img, cmap='gray_r');\n",
        "\n",
        "    # Allow use of tensor using no gradient to speed up process\n",
        "    with torch.no_grad():\n",
        "        # log probability from model of the image\n",
        "        logpb = model(img_tr_new)\n",
        "    \n",
        "    # convert log probability to exponetial to get acutal value\n",
        "    pb = torch.exp(logpb)\n",
        "    # Will return an array of probabilites for each digit\n",
        "    probab = list(pb.numpy()[0])\n",
        "\n",
        "    return probab"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SewAvfAabKhD",
        "outputId": "d1813625-e79d-44ae-b141-2887acf24d83"
      },
      "source": [
        "# Function calls\n",
        "\n",
        "# Load and transform the MNIST data set\n",
        "images, labels, train_loader, test_loader = load_data()\n",
        "# Build the model\n",
        "model = build_Model()\n",
        "# Create the loss function\n",
        "loss, loss_function = loss_function(images,labels, train_loader, model)\n",
        "# Function to calculate the gradient of descent and weights\n",
        "grad_weights(loss, model)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, loss_function)\n",
        "\n",
        "#Validate the model\n",
        "validate(test_loader, model)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "building model... \n",
            "\n",
            "Model built: \n",
            "\t 1) 1st layer: Linear (in_ features = 784, out_features = 128)\n",
            "\t \t Activation function for 1st layer = ReLU()\n",
            "\t 2) 2nd layer: Linear (in_ features = 128, out_features = 64)\n",
            "\t \t Activation function for 2nd layer = ReLU()\n",
            "\t 3) 3rd layer: Linear (in_ features = 64, out_features = 10)\n",
            "\t \t Activation function for 3rd layer = LogSoftmax() \n",
            "\n",
            "defining loss function...\n",
            "Training neural network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 -> Training loss = 0.3076688815166415\n",
            "Epoch 1 -> Training loss = 0.1034963679885162\n",
            "Epoch 2 -> Training loss = 0.06730567892221039\n",
            "Epoch 3 -> Training loss = 0.04874701128785076\n",
            "Epoch 4 -> Training loss = 0.035006257245810565\n",
            "Epoch 5 -> Training loss = 0.027012230864210205\n",
            "Epoch 6 -> Training loss = 0.019589222667433237\n",
            "Epoch 7 -> Training loss = 0.014289511216016038\n",
            "Epoch 8 -> Training loss = 0.009419546798670238\n",
            "Epoch 9 -> Training loss = 0.00635480594989971\n",
            "Epoch 10 -> Training loss = 0.0051852714294558675\n",
            "Epoch 11 -> Training loss = 0.0028738472737459544\n",
            "Epoch 12 -> Training loss = 0.0016595835239830753\n",
            "Epoch 13 -> Training loss = 0.001084862771744726\n",
            "Epoch 14 -> Training loss = 0.0009601275226085486\n",
            "Epoch 15 -> Training loss = 0.0007059259361475208\n",
            "Epoch 16 -> Training loss = 0.0006004877079970517\n",
            "Epoch 17 -> Training loss = 0.0005362659363970313\n",
            "Epoch 18 -> Training loss = 0.000473842693755907\n",
            "Epoch 19 -> Training loss = 0.00043514985831423275\n",
            "Epoch 20 -> Training loss = 0.0003984002247163714\n",
            "Epoch 21 -> Training loss = 0.0003672291566075736\n",
            "Epoch 22 -> Training loss = 0.0003393948651303953\n",
            "Epoch 23 -> Training loss = 0.0003211727406820357\n",
            "Epoch 24 -> Training loss = 0.00029755921838583543\n",
            "Epoch 25 -> Training loss = 0.00028433275473484123\n",
            "Epoch 26 -> Training loss = 0.0002682333273617205\n",
            "Epoch 27 -> Training loss = 0.00025293434032537255\n",
            "Epoch 28 -> Training loss = 0.00024052173263249655\n",
            "Epoch 29 -> Training loss = 0.00022883030931073652\n",
            "\n",
            "Training Time (in minutes) = 5.451261166731516\n",
            "\n",
            "Validating model...\n",
            "Number Of Images Tested = 10000\n",
            "Model Accuracy = 0.9845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "KxLNnFrw1S0u",
        "outputId": "2832c30c-12b7-4496-fc64-2c693a32aacf"
      },
      "source": [
        "print(\"Done! \\n\")\n",
        "\n",
        "# Blank path for first condition of while loop\n",
        "path = \"\"\n",
        "\n",
        "while True:\n",
        "    # Get the image at the file path entered\n",
        "    print(\"Please enter a filepath:\")\n",
        "    path = input()\n",
        "    \n",
        "    if path == 'exit':\n",
        "      break\n",
        "      \n",
        "    # Call the classifciation function\n",
        "    probab = classify_image(path, model)\n",
        "    \n",
        "    # The classification of the digit is the max probabilty from out model\n",
        "    print(\"Classifier =\", probab.index(max(probab)))\n",
        "\n",
        "    # Show the image\n",
        "    img = Image.open(path)\n",
        "    plt.imshow(img, cmap='gray_r');\n",
        "\n",
        "print(\"exiting...\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done! \n",
            "\n",
            "Please enter a filepath:\n",
            "/content/Thick/level08thick.jpg\n",
            "Classifier = 0\n",
            "Please enter a filepath:\n",
            "/content/Thick/lev28thick.jpg\n",
            "Classifier = 0\n",
            "Please enter a filepath:\n",
            "/content/Thick/lev36thick.jpg\n",
            "Classifier = 6\n",
            "Please enter a filepath:\n",
            "/content/Thick/lev26thick.jpg\n",
            "Classifier = 7\n",
            "Please enter a filepath:\n",
            "/content/Thick/lev27thick.jpg\n",
            "Classifier = 3\n",
            "Please enter a filepath:\n",
            "exit\n",
            "exiting...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgGwXt-kfST8"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(images[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(labels[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCs3gZCvfTjt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydbPfL4TfbE1"
      },
      "source": [
        "torch.save(model, 'checkpoint.pth')\n",
        "# download checkpoint file\n",
        "\n",
        "img = Image.open('/content/Testing_Own/img2.jpg')\n",
        "\n",
        "# Create a transform for the image\n",
        "# Convert it to a tensor\n",
        "t = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=1)\n",
        "])\n",
        "  \n",
        "# Transform the image\n",
        "img_tr = t(img)\n",
        "# Flatten to 1D by 784 pixels\n",
        "img_tr_new = img_tr.view(1, 784)\n",
        "\n",
        "plt.imshow(img, cmap='gray_r');"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}